{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from skimage.feature import hog\n",
    "from skimage.io import imread\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import match_descriptors, plot_matches, SIFT\n",
    "from skimage import transform\n",
    "\n",
    "\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from PIL import Image \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../scripts/')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "from svm_functions import resize_all, RGB2GrayTransformer, HogTransformer\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data (pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = 'Respiratory_Disease_Classification'\n",
    "cwd_path = os.getcwd()\n",
    "train_dir = cwd_path.split(repo_name, 1)[0]+repo_name+\"/data/images/90-10_split/train\"\n",
    "src = os.listdir(train_dir)[1:]\n",
    "\n",
    "base_name_train = 'spectrogram_train'\n",
    "base_name_val = 'spectrogram_val'\n",
    "\n",
    "width = 80\n",
    "include = src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples:  2491\n",
      "number of validation samples:  998\n",
      "keys:  ['description', 'label', 'filename', 'data']\n",
      "description:  resized (80x80)animal images in rgb\n",
      "image shape:  (80, 80, 3)\n",
      "labels: ['0' '1']\n",
      "Training data Counter({'0': 1508, '1': 983})\n",
      "Validation data Counter({'0': 674, '1': 324})\n"
     ]
    }
   ],
   "source": [
    "data_train = joblib.load(f'{base_name_train}_{width}x{width}px.pkl')\n",
    "data_val = joblib.load(f'{base_name_val}_{width}x{width}px.pkl')\n",
    " \n",
    "print('number of training samples: ', len(data_train['data']))\n",
    "print('number of validation samples: ', len(data_val['data']))\n",
    "\n",
    "print('keys: ', list(data_train.keys()))\n",
    "print('description: ', data_train['description'])\n",
    "print('image shape: ', data_train['data'][0].shape)\n",
    "print('labels:', np.unique(data_train['label']))\n",
    " \n",
    "print(\"Training data\",Counter(data_train['label']))\n",
    "print(\"Validation data\", Counter(data_val['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train / Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Train-Test, use validation data for testing\n",
    "X_train = np.array(data_train['data'])\n",
    "y_train = np.array(data_train['label'])\n",
    "X_test = np.array(data_val['data'])\n",
    "y_test = np.array(data_val['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary pre-processing kernels\n",
    "1. HOG (histogram oriented gradients)\n",
    "2. SIFT\n",
    "3. SURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inistantiate\n",
    "rgb2gray = RGB2GrayTransformer()\n",
    "hogify = HogTransformer(\n",
    "        pixels_per_cell=(14, 14), \n",
    "        cells_per_block=(2,2), \n",
    "        orientations=9, \n",
    "        block_norm='L2-Hys'\n",
    "        )\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct:  74.14829659318637\n"
     ]
    }
   ],
   "source": [
    "HOG_pipeline = Pipeline([\n",
    "    ('grayify', RGB2GrayTransformer()),\n",
    "    ('hogify', HogTransformer(\n",
    "        pixels_per_cell=(14, 14), \n",
    "        cells_per_block=(2, 2), \n",
    "        orientations=9, \n",
    "        block_norm='L2-Hys')\n",
    "    ),\n",
    "    ('scalify', StandardScaler()),\n",
    "    ('classify', SGDClassifier(random_state=42, max_iter=1000, tol=1e-3))\n",
    "])\n",
    " \n",
    "clf = HOG_pipeline.fit(X_train, y_train)\n",
    "print('Percentage correct: ', 100*np.sum(clf.predict(X_test) == y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "param_grid = [\n",
    "    {\n",
    "        'hogify__orientations': [8, 9],\n",
    "        'hogify__cells_per_block': [(2, 2), (3, 3)],\n",
    "        'hogify__pixels_per_cell': [(8, 8), (10, 10), (12, 12)]\n",
    "    },\n",
    "    {\n",
    "        'hogify__orientations': [8],\n",
    "         'hogify__cells_per_block': [(3, 3)],\n",
    "         'hogify__pixels_per_cell': [(8, 8)],\n",
    "         'classify': [\n",
    "             SGDClassifier(random_state=42, max_iter=1000, tol=1e-3),\n",
    "             svm.SVC(kernel='linear')\n",
    "         ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(HOG_pipeline, \n",
    "                           param_grid, \n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           scoring='accuracy',\n",
    "                           verbose=1,\n",
    "                           return_train_score=True)\n",
    " \n",
    "grid_res = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m X_test_gray \u001b[39m=\u001b[39m rgb2gray\u001b[39m.\u001b[39mtransform(X_test)\n\u001b[1;32m      2\u001b[0m X_test_hog \u001b[39m=\u001b[39m hogify\u001b[39m.\u001b[39mtransform(X_test_gray)\n\u001b[0;32m----> 3\u001b[0m X_test_prepared \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mtransform(X_test_hog)\n",
      "File \u001b[0;32m~/neuefische/Respiratory_Disease_Classification/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:970\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X, copy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    956\u001b[0m     \u001b[39m\"\"\"Perform standardization by centering and scaling.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \n\u001b[1;32m    958\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39m        Transformed array.\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    972\u001b[0m     copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m    973\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    974\u001b[0m         X,\n\u001b[1;32m    975\u001b[0m         reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    980\u001b[0m         force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    981\u001b[0m     )\n",
      "File \u001b[0;32m~/neuefische/Respiratory_Disease_Classification/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:1222\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     fitted \u001b[39m=\u001b[39m [\n\u001b[1;32m   1218\u001b[0m         v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mvars\u001b[39m(estimator) \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1219\u001b[0m     ]\n\u001b[1;32m   1221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1222\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "X_test_gray = rgb2gray.transform(X_test)\n",
    "X_test_hog = hogify.transform(X_test_gray)\n",
    "X_test_prepared = scaler.transform(X_test_hog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f22d6e1ed2f07f99891de3deced0b89877131ec4e9ef94974ed162ef8311fd50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
